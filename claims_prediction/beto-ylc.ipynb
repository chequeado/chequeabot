{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_features_entities_rasa import RasaExtractor\n",
    "\n",
    "ex = RasaExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cargar Rasa + duckling\n",
    "RASA_HOST = '127.0.0.1'\n",
    "RASA_PORT = 5005\n",
    "\n",
    "rasa_endpoint = \"http://{0}:{1}/model/parse\".format(RASA_HOST, RASA_PORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cargo los claims\n",
    "with open('data/claims.json') as json_file: \n",
    "        claims = json.load(json_file) \n",
    "\n",
    "with open('data/output_all.jsonl', 'r') as json_file: \n",
    "    json_list = list(json_file)\n",
    "\n",
    "sentences = [json.loads(sentence) for sentence in json_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/RuPERTa-base-finetuned-pawsx-es\")\n",
    "#model = AutoModelForSequenceClassification.from_pretrained(\"mrm8488/RuPERTa-base-finetuned-pawsx-es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_b = AutoTokenizer.from_pretrained(\"mrm8488/electricidad-base-finetuned-pawsx-es\")\n",
    "model_b = AutoModelForSequenceClassification.from_pretrained(\"mrm8488/electricidad-base-finetuned-pawsx-es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n",
      "...\n",
      "...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'unit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-aa28a0175272>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sentence_text\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mse_ents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_claim_entities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0msimilarity_dicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcl_ents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mse_ents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msimilarity_dicts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'matches_points'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/chequeabot/claims_prediction/extract_features_entities_rasa.py\u001b[0m in \u001b[0;36moutput\u001b[0;34m(self, ent_claim_a, ent_claim_b)\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m             match_num = getattr(self, ENTITIES_DATA[ent_type_a][\"match_type\"])(\n\u001b[0m\u001b[1;32m    220\u001b[0m                 \u001b[0ment_data_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ment_data_b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             )\n",
      "\u001b[0;32m~/projects/chequeabot/claims_prediction/extract_features_entities_rasa.py\u001b[0m in \u001b[0;36mclose_match\u001b[0;34m(self, ent_data_a, ent_data_b)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0ment_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ment_data_a\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"entity\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0ment_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"amount-of-money\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m             \u001b[0mmatches_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_match_money\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ment_data_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ment_data_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0ment_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"number\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/chequeabot/claims_prediction/extract_features_entities_rasa.py\u001b[0m in \u001b[0;36mclose_match_money\u001b[0;34m(self, ent_data_a, ent_data_b)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ment_a\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ment_data_a\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0mcoin_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ment_a\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"additional_info\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"unit\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0mamount_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ment_a\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"additional_info\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0ment_b\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ment_data_b\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'unit'"
     ]
    }
   ],
   "source": [
    "for claim in claims:\n",
    "    claim = claim.get(\"claim\")\n",
    "    cl_ents = ex.get_claim_entities(claim)\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.get(\"sentence_text\")\n",
    "        se_ents = ex.get_claim_entities(sentence)\n",
    "        similarity_dicts = ex.output(cl_ents, se_ents)\n",
    "        if similarity_dicts['matches_points'] >= 1:\n",
    "            \n",
    "            claims_pair = tokenizer_b.encode_plus(claim, sentence, return_tensors=\"pt\")\n",
    "            claims_pair_logits = model_b(**claims_pair)[0]\n",
    "            claims_pair_results = torch.softmax(claims_pair_logits, dim=1).tolist()[0]\n",
    "            if claims_pair_results[1] > 0.7:\n",
    "                import pdb; pdb.set_trace()\n",
    "                \n",
    "    print ('...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should be paraphrase\n",
      "no son lo mismo: 19%\n",
      "son lo mismo: 81%\n",
      "\n",
      "Should not be paraphrase\n",
      "no son lo mismo: 66%\n",
      "son lo mismo: 34%\n"
     ]
    }
   ],
   "source": [
    "classes = [\"no son lo mismo\", \"son lo mismo\"]\n",
    "\n",
    "sequence_0 = \"La inflación en 2016 fue de 50%\"\n",
    "sequence_1 = \"En el 2016 tuvimos una inflación cercana al 50%\"\n",
    "sequence_2 = \"La inflación es un problema que Argentina no puede solucionar\"\n",
    "\n",
    "paraphrase = tokenizer.encode_plus(sequence_0, sequence_1, return_tensors=\"pt\")\n",
    "not_paraphrase = tokenizer.encode_plus(sequence_0, sequence_2, return_tensors=\"pt\")\n",
    "\n",
    "paraphrase_classification_logits = model(**paraphrase)[0]\n",
    "not_paraphrase_classification_logits = model(**not_paraphrase)[0]\n",
    "\n",
    "paraphrase_results = torch.softmax(paraphrase_classification_logits, dim=1).tolist()[0]\n",
    "not_paraphrase_results = torch.softmax(not_paraphrase_classification_logits, dim=1).tolist()[0]\n",
    "\n",
    "print(\"Should be paraphrase\")\n",
    "for i in range(len(classes)):\n",
    "    print(f\"{classes[i]}: {round(paraphrase_results[i] * 100)}%\")\n",
    "\n",
    "print(\"\\nShould not be paraphrase\")\n",
    "for i in range(len(classes)):\n",
    "    print(f\"{classes[i]}: {round(not_paraphrase_results[i] * 100)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_pair = tokenizer.encode_plus(sequence_0, sequence_1, return_tensors=\"pt\")\n",
    "claims_pair_logits = model(**claims_pair)[0]\n",
    "claims_pair_results = torch.softmax(claims_pair_logits, dim=1).tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no son lo mismo: 19%\n",
      "son lo mismo: 81%\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(classes)):\n",
    "    print(f\"{classes[i]}: {round(claims_pair_results[i] * 100)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.18940554559230804, 0.8105944991111755]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "claims_pair_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims = [\n",
    "    'La inflación en 2016 fue de 50%',\n",
    "    'En el 2016 tuvimos una inflación cercana al 50%',\n",
    "    'La inflación es un problema que Argentina no puede solucionar',\n",
    "    'En los 12 últimos meses la inflación fue la más baja de los últimos siete años.',\n",
    "    'Hay que trabajar medio año para el Estado.',\n",
    "    'Trabajamos la mitad del año para pagarle impuestos al Estado',\n",
    "    'Entre 2003 y 2011 creamos 300 mil empleos.',\n",
    "    'En la industria generamos más de un millón de puestos de trabajo.',\n",
    "    'Creamos más de un millón de empleos en la industria',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_claims = []\n",
    "for i in range(len(claims)):\n",
    "    for j in range(len(claims)):\n",
    "        if i == j:\n",
    "            continue\n",
    "        claims_pair = tokenizer.encode_plus(claims[i], claims[j], return_tensors=\"pt\")\n",
    "        claims_pair_logits = model(**claims_pair)[0]\n",
    "        claims_pair_results = torch.softmax(claims_pair_logits, dim=1).tolist()[0]\n",
    "        if claims_pair_results[1] > 0.7:\n",
    "            similar_claims.append([claims[i], claims[j], claims_pair_results[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_claims_b = []\n",
    "for i in range(len(claims)):\n",
    "    for j in range(len(claims)):\n",
    "        if i == j:\n",
    "            continue\n",
    "        claims_pair = tokenizer_b.encode_plus(claims[i], claims[j], return_tensors=\"pt\")\n",
    "        claims_pair_logits = model_b(**claims_pair)[0]\n",
    "        claims_pair_results = torch.softmax(claims_pair_logits, dim=1).tolist()[0]\n",
    "        if claims_pair_results[1] > 0.7:\n",
    "            similar_claims_b.append([claims[i], claims[j], claims_pair_results[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['La inflación en 2016 fue de 50%',\n",
       "  'En el 2016 tuvimos una inflación cercana al 50%',\n",
       "  0.8105944991111755],\n",
       " ['En el 2016 tuvimos una inflación cercana al 50%',\n",
       "  'La inflación en 2016 fue de 50%',\n",
       "  0.806519627571106],\n",
       " ['Trabajamos la mitad del año para pagarle impuestos al Estado',\n",
       "  'La inflación en 2016 fue de 50%',\n",
       "  0.7567902207374573],\n",
       " ['Trabajamos la mitad del año para pagarle impuestos al Estado',\n",
       "  'Hay que trabajar medio año para el Estado.',\n",
       "  0.721567690372467]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['La inflación en 2016 fue de 50%',\n",
       "  'En el 2016 tuvimos una inflación cercana al 50%',\n",
       "  0.805130660533905],\n",
       " ['En el 2016 tuvimos una inflación cercana al 50%',\n",
       "  'La inflación en 2016 fue de 50%',\n",
       "  0.8785433769226074],\n",
       " ['Hay que trabajar medio año para el Estado.',\n",
       "  'Trabajamos la mitad del año para pagarle impuestos al Estado',\n",
       "  0.7958112359046936],\n",
       " ['En la industria generamos más de un millón de puestos de trabajo.',\n",
       "  'Creamos más de un millón de empleos en la industria',\n",
       "  0.9922629594802856],\n",
       " ['Creamos más de un millón de empleos en la industria',\n",
       "  'En la industria generamos más de un millón de puestos de trabajo.',\n",
       "  0.9160385131835938]]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_claims_b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
