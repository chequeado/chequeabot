{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Descomentar si se necesita instalar alguna de las librerias\n",
    "import glob\n",
    "import pickle\n",
    "import sys\n",
    "#!{sys.executable} -m pip install numpy\n",
    "#!{sys.executable} -m pip install pandas\n",
    "#!{sys.executable} -m pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "RASA_HOST = '127.0.0.1'\n",
    "RASA_PORT = 5005\n",
    "\n",
    "rasa_endpoint = \"http://{0}:{1}/model/parse\".format(RASA_HOST, RASA_PORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(claim):\n",
    "    res = requests.post(rasa_endpoint, json={\"text\": claim})\n",
    "    data = res.json()\n",
    "    present_entities = []\n",
    "    if \"entities\" in data:\n",
    "        for ent in data['entities']:\n",
    "            if ent['entity'] not in present_entities:\n",
    "                present_entities.append(ent['entity'])\n",
    "        return present_entities\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PER', 'GPE', 'MISC']"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_entities('Alberto Fernández es el Presidente de Argentina')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import POS_TAGGED_FOLDER, SPACY_FOLDER\n",
    "sys.path.append(SPACY_FOLDER)  \n",
    "from feature_extractors import automatic_feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tagged_sentences(folder):\n",
    "    # Load all the tagged sentences included in the .pickle files \n",
    "    parsed_sentences = []\n",
    "    for filename in glob.glob(folder + '*.pickle'):\n",
    "        with open(filename, 'rb') as tagged_file:\n",
    "            parsed_sentences = parsed_sentences + pickle.load(tagged_file, encoding=\"bytes\")\n",
    "    return parsed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_sentences = get_tagged_sentences(POS_TAGGED_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'non-fact-checkable'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_sentences[0][b'classification'].decode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Durante al año 2014 numerosos dirigentes internacionales presidentes organismos multilaterales de crédito y organismos multilaterales en general auguraban que el 2014 iba a ser un año donde culminara terminara la crisis iniciada por la caída de Lehman Brothers en el 2008'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_sentences[30][b'sentence'].decode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "presidenta\n"
     ]
    }
   ],
   "source": [
    "print(tagged_sentences[0][b'pos_tag'][3][b'lemma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([b'dep', b'text', b'pos', b'lemma', b'tag', b'like_num', b'is_punct', b'tense'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_sentences[0][b'pos_tag'][3].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "from spacy.lang.es import Spanish\n",
    "\n",
    "stop_words = STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = spacy.load('es_core_news_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag(sentence, pos_tag=False):\n",
    "    \n",
    "    \n",
    "    mytokens = parser(sentence)\n",
    "    tags = []\n",
    "    for token in mytokens:\n",
    "        token_tags = []\n",
    "        \n",
    "        if pos_tag:\n",
    "            pos = token.pos_\n",
    "            token_tags.append(pos) if pos else None\n",
    "        \n",
    "        lemma = token.lemma_.lower()\n",
    "        token_tags.append(lemma) if lemma else None\n",
    "        \n",
    "        #word['dep'] = token.dep_\n",
    "        #is_punct = token.is_punct\n",
    "        #like_num = token.like_num\n",
    "\n",
    "        if \"Tense\" in token.tag_:\n",
    "            trunk = token.tag_[token.tag_.find(\"Tense\") :]\n",
    "            trunk = trunk[trunk.find('=')+1:trunk.find('|')]\n",
    "            tense = trunk\n",
    "            token_tags.append(tense)\n",
    "        else:\n",
    "            tense = \"undefined\"\n",
    "            \n",
    "        named_entities = extract_entities(sentence)\n",
    "\n",
    "        tags.extend(token_tags)\n",
    "        tags.extend(named_entities)\n",
    "\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# Creating our tokenizer function\n",
    "def spacy_tokenizer(sentence):\n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    mytokens = parser(sentence)\n",
    "    \n",
    "    # Lemmatizing each token and converting each token into lowercase\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "\n",
    "    # Removing stop words\n",
    "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
    "    \n",
    "    # return preprocessed list of tokens\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['macri', 'incrementar', 'pobreza']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_tokenizer('Macri incrementó la pobreza')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PROPN',\n",
       " 'macri',\n",
       " 'PER',\n",
       " 'TOPIC',\n",
       " 'VERB',\n",
       " 'incrementar',\n",
       " 'Past',\n",
       " 'PER',\n",
       " 'TOPIC',\n",
       " 'DET',\n",
       " 'lo',\n",
       " 'PER',\n",
       " 'TOPIC',\n",
       " 'NOUN',\n",
       " 'pobreza',\n",
       " 'PER',\n",
       " 'TOPIC']"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag('Macri incrementó la pobreza')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vector = CountVectorizer(tokenizer = pos_tag, ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vector = TfidfVectorizer(tokenizer = pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [{'sentence': item[b'sentence'].decode('utf8').lower(), 'target': item[b'classification'].decode('utf8')} for item in tagged_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(df, df['target']):\n",
    "    strat_train_set = df.loc[train_index]\n",
    "    strat_test_set = df.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "non-fact-checkable    1914\n",
       "fact-checkable        1175\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strat_train_set.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "non-fact-checkable    479\n",
       "fact-checkable        294\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strat_test_set.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = strat_train_set.drop(['target'], axis=1)\n",
    "X_test = strat_test_set.drop(['target'], axis=1)\n",
    "y_train = strat_train_set['target']\n",
    "y_test = strat_test_set['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = [e['sentence'] for e in data]\n",
    "y = [e['target'] for e in data]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(tokenizer = pos_tag, ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.8305882352941176\n",
      "Logistic Precision: 0.8307692307692308\n",
      "Logistic Recall: 0.6835443037974683\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "pipe = Pipeline([\n",
    "            ('vectorizer', CountVectorizer()),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "            ('classifier', classifier)\n",
    "])\n",
    "\n",
    "\n",
    "# model generation\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Predicting with a test dataset\n",
    "predicted = pipe.predict(X_test)\n",
    "\n",
    "# Model Accuracy\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"Logistic Precision:\",metrics.precision_score(y_test, predicted, pos_label='fact-checkable'))\n",
    "print(\"Logistic Recall:\",metrics.recall_score(y_test, predicted, pos_label='fact-checkable'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB Accuracy: 0.8243137254901961\n",
      "MNB Precision: 0.9111842105263158\n",
      "MNB Recall: 0.5843881856540084\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "pipe = Pipeline([\n",
    "        ('vectorizer', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('classifier', classifier)\n",
    "])\n",
    "\n",
    "\n",
    "# model generation\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Predicting with a test dataset\n",
    "predicted = pipe.predict(X_test)\n",
    "\n",
    "# Model Accuracy\n",
    "print(\"MNB Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"MNB Precision:\",metrics.precision_score(y_test, predicted, pos_label='fact-checkable'))\n",
    "print(\"MNB Recall:\",metrics.recall_score(y_test, predicted, pos_label='fact-checkable'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fact-checkable'"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vector = TfidfVectorizer(tokenizer = pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB Accuracy: 0.8486274509803922\n",
      "MNB Precision: 0.81859410430839\n",
      "MNB Recall: 0.7616033755274262\n"
     ]
    }
   ],
   "source": [
    "classifier = MultinomialNB()\n",
    "\n",
    "pipe = Pipeline([\n",
    "                ('vectorizer', CountVectorizer()),\n",
    "                ('classifier', classifier)])\n",
    "\n",
    "\n",
    "# model generation\n",
    "pipe.fit(X_train,y_train)\n",
    "\n",
    "# Predicting with a test dataset\n",
    "predicted = pipe.predict(X_test)\n",
    "\n",
    "# Model Accuracy\n",
    "print(\"MNB Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"MNB Precision:\",metrics.precision_score(y_test, predicted, pos_label='fact-checkable'))\n",
    "print(\"MNB Recall:\",metrics.recall_score(y_test, predicted, pos_label='fact-checkable'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Accuracy: 0.8250980392156863\n",
      "SGD Precision: 0.8177215189873418\n",
      "SGD Recall: 0.6814345991561181\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "classifier = SGDClassifier(loss='hinge', penalty='l2',\n",
    "                           alpha=1e-3, random_state=42,\n",
    "                           max_iter=5, tol=None)\n",
    "\n",
    "pipe = Pipeline([\n",
    "        ('vectorizer', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('classifier', classifier)\n",
    "])\n",
    "\n",
    "\n",
    "# model generation\n",
    "pipe.fit(X_train,y_train)\n",
    "\n",
    "# Predicting with a test dataset\n",
    "predicted = pipe.predict(X_test)\n",
    "\n",
    "# Model Accuracy\n",
    "print(\"SGD Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"SGD Precision:\",metrics.precision_score(y_test, predicted, pos_label='fact-checkable'))\n",
    "print(\"SGD Recall:\",metrics.recall_score(y_test, predicted, pos_label='fact-checkable'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
